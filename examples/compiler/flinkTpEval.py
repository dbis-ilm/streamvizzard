import ast
import os
import statistics
import sys
from typing import List, Optional, Dict

import matplotlib
import matplotlib.pyplot as plt

import numpy as np

# This evaluation scripts calculates the throughput per processed tuple based on the output generated by a flink data sink.
# In addition, the produced result values are analyzed and may be compared to results of other runs to verify semantic correctness.
# The result file loaded by this script must contain a Python tuple with a numeric data value at index 0 and a monotonic time value at index 1 per line.
# The time value represents the timestamp when a data tuple was produced by the source operator.
# E.g.: (1, 126.123)\n(2.5, 130.32),...
# The last operator in the pipeline before the data sink should therefore execute on a single node to ensure monotonic times.

matplotlib.use('Qt5Agg')  # or 'TkAgg ' or 'MacOSX' depending on your OS | Install required dependencies!

inputDataPath: str = "scenarios/energyMarket/output_sv.txt"
compareWithDataPaths: List[str] = ["scenarios/energyMarket/output_flink_5para.txt"]  # May contain multiple Flink execution results, e.g. with different level of parallelism

# The output files should contain a Python tuple with a data and time value at the following indices:
dataIdx = 0
timestampIdx = 1

# The minimum required sliding window size [seconds] for the tp calculation to reduce high/low peaks for few tuples
minSlidingWindowDuration: float = 5

# The amount of tuples to include during smoothening of the calculated throughputs of the overall execution
smoothSize: int = 0

# The amount of tuples [percentage] of the input data file to consider as warmup/warmdown and skip during further calculations
skipWarmUpTuples: float = 0.1  # [0,1]
skipWarmDownTuples: float = 0.1  # [0,1]

# The maximum number of result values to plot or export. Exceeding values will be dropped equally (modulo)
maxResultVals: Optional[int] = None

plotTps = True
plotData = True
compareData = True

# Folder to export the calculated throughput results [omit, if no export desired]
resTpFolder: Optional[str] = None
resDataFolder: Optional[str] = None


# ----------------------------------------------------------------------------------------------------------------------

def readFile(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.readlines()
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return None


def getFileStats(filePath: str):
    print(f"Retrieving stats from file {filePath} ...")

    linesRaw = readFile(filePath)

    if linesRaw is None:
        sys.exit(1)

    lines = [ast.literal_eval(line) for line in linesRaw]

    # Collect data [dont consider warmup]

    vals: List[float] = [d[dataIdx] for d in lines]

    skipWarmUpIdx = int(len(lines) * skipWarmUpTuples)
    skipWarmDownIdx = len(lines) - int(len(lines) * skipWarmDownTuples)

    origLines = lines.copy()
    lines = lines[skipWarmUpIdx:skipWarmDownIdx]

    # Collect tps

    def unifiedTimestamp(ts: float):
        # Reduce timestamp to ms accuracy to group batch arriving tuples (sub-ms range)
        return round(ts, 3)

    # Group tuple entries by timestamps (if multiple tuples arrive at same time)

    timeGrouping: Dict[float, int] = dict()

    for line in lines:
        timestamp = unifiedTimestamp(line[timestampIdx])

        groupedLines = timeGrouping.get(timestamp, 0)
        groupedLines += 1

        timeGrouping[timestamp] = groupedLines

    # Calculate throughputs

    tps: List[float] = []
    relTimes: List[float] = []

    timestampKeys = sorted(timeGrouping)
    firstTimestamp = timestampKeys[0]

    # We calculate the throughput as a sliding window which considers the whole produced tuples at each timestamp

    slidingTotalTuples = 0

    for timestamp in timestampKeys:
        totalTuples = timeGrouping[timestamp]

        slidingTotalTuples += totalTuples

        # Calculate throughput based on sliding window

        delta = (timestamp - firstTimestamp)

        if delta < minSlidingWindowDuration or delta <= 0:
            continue

        tp = slidingTotalTuples / delta

        # Add stats for each tuple in the grouping

        for _ in range(totalTuples):
            tps.append(tp)
            relTimes.append(timestamp - firstTimestamp)

    # For the whole data set

    totalElapsed = origLines[-1][timestampIdx] - origLines[0][timestampIdx]
    warmUpElapsed = lines[-1][timestampIdx] - lines[0][timestampIdx]

    # For reordered ops the tp might jump in the end to batch watermark progress after source gets idle?

    if len(tps) < 2:
        print("Error: Not enough calculated throughput points, window size / warmup to big?")

        sys.exit(-1)

    avgCalcTp = statistics.mean(tps)  # Real calculated avg tp

    print(f">>> Produced {len(origLines)} ({len(lines)}) tuples in {round(totalElapsed, 2)} s ({round(warmUpElapsed, 2)} s)| "
          f"Total Avg Tp: {round(len(origLines) / totalElapsed, 2)} | "
          f"Warmed Up Avg Tp: {round(len(lines) / warmUpElapsed, 2)} | "
          f"Calc. Avg Tp: {round(avgCalcTp, 2)}")

    if smoothSize > 0:
        smoothedTps = np.convolve(tps, np.ones(smoothSize) / smoothSize, mode='valid')
    else:
        smoothedTps = tps

    return smoothedTps, vals, relTimes


def getElmsToWrite(data):
    lines = []

    maxElms = max(1, (maxResultVals if maxResultVals is not None else len(data)) - 1)

    writeEvery = max(1, int(len(data) / maxElms))

    for i in range(0, len(data), writeEvery):
        lines.append(data[i])

        if len(lines) >= maxElms:
            break

    lines.append(data[-1])  # Include last val

    return lines


def main():
    origName = os.path.splitext(os.path.basename(inputDataPath))[0]
    tps, vals, relTimes = getFileStats(inputDataPath)

    otherTps = None
    otherVals = None

    if compareWithDataPaths is not None:
        otherTps = []
        otherVals = []

        for compareWithDataPath in compareWithDataPaths:
            name = os.path.splitext(os.path.basename(compareWithDataPath))[0]

            ot, ov, ort = getFileStats(compareWithDataPath)

            otherTps.append(ot)
            otherVals.append(ov)

            # Compare data

            if compareData:
                matching = True

                inOrder = 0

                # Detect how many vals are in order

                for i in range(min(len(otherVals), len(vals))):
                    ourVal = vals[i]
                    otherVal = ov[i]

                    if ourVal != otherVal:
                        matching = False
                    else:
                        inOrder += 1

                globalMatches = 0

                remainingOrigVals = vals[:]  # Copy

                # For global order, some of the vals we check might not be produced yet (at the end of the data set)

                checkL = int(len(ov) * 0.85)

                for i in range(checkL):
                    otherVal = ov[i]

                    if otherVal in remainingOrigVals:
                        remainingOrigVals.remove(otherVal)
                        globalMatches += 1

                if not matching:
                    print(f"Data {name} does not match {origName}! Global Matches: {round(globalMatches / checkL * 100, 2)}% | Tups in Order: {round(inOrder / len(vals) * 100, 2)}%")
                else:
                    print(f"Data {name} matches {origName}!")

    reducedTpIdx = getElmsToWrite([i for i in range(len(tps))])
    reducedTps = getElmsToWrite(tps)
    reducedDataIdx = getElmsToWrite([i for i in range(len(vals))])
    reducedData = getElmsToWrite(vals)

    # Store tp

    if resTpFolder is not None:
        with open(os.path.join(resTpFolder, "tp_" + os.path.splitext(os.path.basename(inputDataPath))[0] + ".txt"), "w") as file:
            file.write("x,y\n")
            file.writelines([f"{reducedTpIdx[i]}, {d}\n" for i, d in enumerate(reducedTps)])

        if otherTps is not None:
            for idx in range(len(otherTps)):
                otherFileName = os.path.splitext(os.path.basename(compareWithDataPaths[idx]))[0]

                otherReducedTp = getElmsToWrite(otherTps[idx])
                otherReducedIdx = getElmsToWrite([i for i in range(len(otherTps[idx]))])

                with open(os.path.join(resTpFolder, "tp_" + str(otherFileName) + ".txt"), "w") as file:
                    file.write("x,y\n")
                    file.writelines([f"{otherReducedIdx[i]}, {d}\n" for i, d in enumerate(otherReducedTp)])

    # Store data

    if resDataFolder is not None:
        with open(os.path.join(resDataFolder, "data_" + os.path.splitext(os.path.basename(inputDataPath))[0] + ".txt"), "w") as file:
            reducedDataIdx = getElmsToWrite([i for i in range(len(vals))])

            file.write("x,y\n")
            file.writelines([f"{reducedDataIdx[i]}, {d}\n" for i, d in enumerate(reducedData)])

        if otherVals is not None:
            for idx in range(len(otherTps)):
                otherFileName = os.path.splitext(os.path.basename(compareWithDataPaths[idx]))[0]

                otherReducedData = getElmsToWrite(otherVals[idx])
                otherReducedIdx = getElmsToWrite([i for i in range(len(otherVals[idx]))])

                with open(os.path.join(resDataFolder, "data_" + str(otherFileName) + ".txt"), "w") as file:
                    file.write("x,y\n")
                    file.writelines([f"{otherReducedIdx[i]}, {d}\n" for i, d in enumerate(otherReducedData)])

    # Plot tp

    if plotTps:
        fileName = os.path.splitext(os.path.basename(inputDataPath))[0]

        plt.plot([i for i in reducedTpIdx], reducedTps, marker='o', label=fileName)

        if otherTps is not None:
            for idx in range(len(otherTps)):
                otherFileName = os.path.splitext(os.path.basename(compareWithDataPaths[idx]))[0]

                otherReduced = getElmsToWrite(otherTps[idx])
                otherReducedIdx = getElmsToWrite([i for i in range(len(otherTps[idx]))])

                plt.plot([i for i in otherReducedIdx], otherReduced, marker='o', label=otherFileName)

        plt.xlabel('Tuple Nr.')
        plt.ylabel('Throughput')
        plt.title('Pipeline Execution Throughput Plot')
        plt.grid(True)
        plt.legend()
        plt.show()

    # Plot data

    if plotData:
        fileName = os.path.splitext(os.path.basename(inputDataPath))[0]

        plt.plot([i for i in reducedDataIdx], reducedData, marker='o', label=fileName)

        if otherTps is not None:
            for idx in range(len(otherVals)):
                otherFileName = os.path.splitext(os.path.basename(compareWithDataPaths[idx]))[0]
                otherReduced = getElmsToWrite(otherVals[idx])
                otherReducedIdx = getElmsToWrite([i for i in range(len(otherVals[idx]))])

                plt.plot([i for i in otherReducedIdx], otherReduced, marker='x', label=otherFileName)

        plt.xlabel('Tuple Nr.')
        plt.ylabel('Data')
        plt.title('Pipeline Execution Data Plot')
        plt.grid(True)
        plt.legend()
        plt.show()


if __name__ == "__main__":
    main()
